"""Investigate the influence and importance of raw data representations (such as inter-arrival time (IAT) and packet
sizes) of IoT traffic to outlier detection.

Outlier detection algorithms:
    OCSVM, KDE, IF, GMM, PCA, and AE.

Measure metric:
    The area under the ROC curve (AUC).

Experiments:
    Exp 1:
        The AUC results generated by each algorithm with default parameters, i.e. using the rule of thumb
        (empirical rule) as default parameters for each algorithm to obtain the results.
    Exp 2:
        The AUC results generated by each algorithm with best parameters.

Note:
    ***Please do not use "optimize imports" in pycharm to this file, because this function will the imported order
of customized packages or libraries, such as "_config".

Default rules:
    0. Think twice before you act!
    1. One function only do one thing and try its best to do the thing well.
    2. Try to use "classes" instead of "functions" and initialize all the stuffs in the __init__() if possible.
    3. Use a clearer and shorter function name rather than a longer one, and make all detail into the function
        "docstring".
    4. Use a meaningful and clear variable name, and try to avoid a long name.
    5. Add as much as details to everything, such as class, function, and even import, and make your work more clearer
        and easier to readers.
    6. Save as much results (such as middle results) as possible to disk to avoid any unexpected exceptions happen

"""
# Authors: kun.bj@outlook.com
#
# License: GNU GENERAL PUBLIC LICENSE

# At the very beginning of the main entrance: "set random seeds and basic parameters to make experimental results
# reproducible.
import os, sys

lib_path = os.path.abspath('../examples/')
sys.path.append(lib_path)
print(f"add \'{lib_path}\' into sys.path: {sys.path}")

try:
    # if os.path.exists('_config.py'):
    from reprst._config_reprst.py import *

    print('from _config import *')
except:
    from itod._config import *

    print('from itod.config import *')

# 1. System and built-in libraries
import argparse
import traceback
import os.path as pth

# 2. Third-party libraries
# such as numpy and pyod
# import pretty_errors  # please do not remove it because it helps to locate errors.

# 3. Local libraries
# focus on data processes, which contains pcap2flows, flow2features, and so on.
from itod.data.data_factory import DataFactory
# highlight on outlier detection algorithms
from itod.ndm.detector_factory import DetectorFactory
# highlight on data visual
from itod.visual.display_factory import DisplayFactory
# include useful tools, such as save data
# include results postprocess, such as results2xlsx and results2latex.
from itod.visual.postprocessing.latex_csv_xlsx import *


class MainFactory:
    """Includes an complete outlier detection procedure of each algorithm with each combination of parameters,
    such as DataFactory, DetectorFactory, and DisplayFactory
    """

    def __init__(self, params={}):
        """Initialize all parameters

        Parameters
        ----------
        params: dict
            all the parameters used in each procedure, and also will be updated during the procedure.

            "params" uses "dict" because so many parameters are includes in params

        """
        self.params = params
        pprint(self.params, name=MainFactory.__name__)  # print common parameters

        self.dataset_name = self.params['dataset_name']
        self.detector_name = self.params['detector_name']

    @func_notation
    def run(self):
        """Execute function

        Returns
        -------
        dp.data_lst: list
            AUC results stored in a list

        """
        print("1. Data process")
        self.dt = DataFactory(dataset_name=self.dataset_name, params=self.params)  # update params
        self.dt.run()  # 1) pcap2features, 2) get train set and test set 3) update params

        print("2. Algorithm analysis")
        # Normalization should be done in this part rather than DataFactory because some algorithms might not require
        # normalization
        self.dtr = DetectorFactory(detector_name=self.detector_name, dataset_inst=self.dt.dataset_inst,
                                   params=self.params)  # update params
        self.dtr.run()  # Execute algorithm

        print("3. Result display")
        self.dp = DisplayFactory(dataset_inst=self.dtr.dataset_inst, params=self.params)
        self.dp.run()

        return self.dp.data_lst


class Parameter:
    """All experimental parameters

    """

    def __init__(self, random_state=42, verbose=1, display=True, overwrite=False):
        """Common parameters

        Parameters
        ----------
        random_state : int
            make experimental results reproducible

        verbose: int
            need to be modified in the future version: verbose: Verbosity mode, 0, 1 or 2

        display: boolean, default as True
            display figures or not.

        overwrite: boolean, default as False

        """
        self.random_state = random_state
        self.verbose = verbose
        self.display = display
        self.overwrite = overwrite

    def get_params(self):
        pass

    def add(self, params={}):
        """Add new attributes to self

        Parameters
        ----------
        params

        Returns
        -------

        """
        for i, (key, value) in enumerate(params.items()):
            setattr(self, key, value)


class DSP(Parameter):
    def __init__(self, ipt_dir='', opt_dir='', dataset_name='', **kwargs):
        """Common dataset parameter

        Parameters
        ----------
        ipt_dir
        opt_dir
        """
        super(DSP, self).__init__()
        self.dataset_name = dataset_name
        self.ipt_dir = "./data/data_reprst/"  # ipt_dir
        self.opt_dir = "../examples/reprst/out/data_reprst/"  # opt_dir
        self.data_cat = 'INDV'  # data category: INDV, AGMT and MIX

        self.q_iat = 0.9
        self.sampling = 'rate'  # sampling method
        for i, (key, value) in enumerate(kwargs.items()):
            setattr(self, key, value)


class DTP(DSP):

    def __init__(self, detector_name='', gs=True, **kwargs):
        """Commom detector parameters

        """
        super(DTP, self).__init__()
        self.detector_name = detector_name
        self.norm = True
        self.norm_method = 'std'
        self.gs = gs  # grid search
        for i, (key, value) in enumerate(kwargs.items()):
            setattr(self, key, value)


class DPP(DTP):

    def __init__(self, title=True, **kwargs):
        """Commom display parameters

        """
        super(DPP, self).__init__()
        self.title = title
        for i, (key, value) in enumerate(kwargs.items()):
            setattr(self, key, value)


class EXPT(DPP):
    """All experimental parameters

    """

    def __init__(self, detector_name='GMM'):
        super(EXPT, self).__init__(detector_name=detector_name)

        # 1. datasets
        self.datasets = [
            # Private IoT: TV&PC
            'DS20_PU_SMTV/DS21-srcIP_10.42.0.1',
            # # Private IoT
            # 'DS60_UChi_IoT/DS61-srcIP_192.168.143.20',
            # 'DS60_UChi_IoT/DS62-srcIP_192.168.143.42',
            # 'DS60_UChi_IoT/DS63-srcIP_192.168.143.43',
            # 'DS60_UChi_IoT/DS64-srcIP_192.168.143.48',
            # # OCS IoT
            # 'DS30_OCS_IoT/DS31-srcIP_192.168.0.13',
            # # CTU IoT: PC&Rsp
            # 'DS40_CTU_IoT/DS41-srcIP_10.0.2.15',
            # # MAWI: 2PCs
            # 'DS50_MAWI_WIDE/DS51-srcIP_202.171.168.50',
            # # # UNB IDS
            # 'DS10_UNB_IDS/DS11-srcIP_192.168.10.5',
            # 'DS10_UNB_IDS/DS12-srcIP_192.168.10.8',
            # 'DS10_UNB_IDS/DS13-srcIP_192.168.10.9',
            # 'DS10_UNB_IDS/DS14-srcIP_192.168.10.14',
            # 'DS10_UNB_IDS/DS15-srcIP_192.168.10.15'
            ## ALL attacks combined in each dataset
            # 'DS10_UNB_IDS/Tuesday/Tuesday-srcIP_172.16.0.1',
            # 'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1',
            # 'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1',
            # 'DS10_UNB_IDS/Thursday/Thursday-srcIP_192.168.10.8',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.5',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.8',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.9',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.12',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.14',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.15',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.17',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_205.174.165.73',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_172.16.0.1',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.50',
            ## Individual Attack
            # # TUESDAY
            # 'DS10_UNB_IDS/Tuesday/Tuesday-srcIP_172.16.0.1/FTP-PATATOR',
            # 'DS10_UNB_IDS/Tuesday/Tuesday-srcIP_172.16.0.1/SSH-PATATOR',
            # # WEDNESDAY
            # 'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-SLOWLORIS',
            # 'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-SLOWHTTPTEST',
            # 'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-HULK',
            # 'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-GOLDENEYE',
            # 'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/HEARTBLEED',
            # # Thursday
            # 'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1/WEB-ATTACK-BRUTE-FORCE',
            # 'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1/WEB-ATTACK-XSS',
            # 'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1/WEB-ATTACK-SQL-INJECTION',
            # 'DS10_UNB_IDS/Thursday/Thursday-srcIP_192.168.10.8/INFILTRATION',
            # Friday
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.5/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.8/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.9/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.12/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.14/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.15/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.17/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_205.174.165.73/BOT',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_172.16.0.1/PORTSCAN',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_172.16.0.1/DDOS',
            # 'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.50/DDOS',
            #
            # # CTU-IoT-23: new IoT dataset
            # 'DS70_CTU_IoT/DS71-srcIP_192.168.100.113',
            # 'DS70_CTU_IoT/DS71-srcIP_192.168.1.198',

        ]  # 'DEMO_IDS/DS-srcIP_192.168.10.5'

        print(f'datasets: {self.datasets}')

        # 'AGMT' for UNB dataset, 'INDV' for the remained datasets
        self.data_cats = ['AGMT', 'INDV']
        print(f'data_cats: {self.data_cats}')

        # default: True, split flows into subflows
        self.subflows = [True]
        # If self.subflow_interval==None, using self.q_flow_dur to obtain the self.subflow_interval in runtime and
        # split flows into subflows  (the detail can be found in data_factory.py).
        self.subflow_interval = None
        if True in self.subflows:
            if type(self.subflow_interval) == float and \
                    (self.subflow_interval > 0) and (self.subflow_interval < 10000) \
                    and (self.subflow_interval != None):
                print(f'+++subflow_interval ({self.subflow_interval}) is preset before the experiment.')
                self.q_flow_dur = None
            else:
                self.q_flow_dur = 0.90  # it's used for calculating the subflow interval in runtime
                print(f'subflow_interval ({self.subflow_interval}) will be calculated in runtime according '
                      f'to q_flow_dur ({self.q_flow_dur})')
            print(f'subflows: {self.subflows}, subflow_interval: {self.subflow_interval}, '
                  f'q_flow_dur: {self.q_flow_dur}')

        # Whether adding header information (such as TTL and TCP flags) into features or not
        self.headers = [False, True]
        print(f'headers: {self.headers}')

        # 2. detector
        self.detectors = [self.detector_name]
        # Whether using grid search or not.
        self.gses = [False, True]

        # 3. display

    def get_dataset_params(self):
        # list(itertools.product(datasets, subflows, headers))
        for i_sf, subflow in enumerate(self.subflows):
            for i_hd, header in enumerate(self.headers):
                for i_dc, data_cat in enumerate(self.data_cats):
                    for i_ds, dataset_name in enumerate(self.datasets):
                        yield {'dataset_name': dataset_name, 'data_cat': data_cat, 'subflow': subflow, 'header': header}

    def get_detector_params(self):
        for i, detector_name in enumerate(self.detectors):
            for j, gs in enumerate(self.gses):
                yield {"detector_name": detector_name, "gs": gs}

    def get_display_params(self):
        titles = [True]
        self.display_params_cnt = len(titles)
        for i, title in enumerate(titles):
            yield DPP(title=title)

    def get_one_comb(self):
        """ get one combination of parameters

        Returns
        -------

        """
        for dsp in self.get_dataset_params():  # get dataset combinations
            for dtp in self.get_detector_params():  # get detector combinations
                self.add(dsp)
                self.add(dtp)
                # del params['datasets']
                # del params['data_cats']
                # del params['subflows']
                # del params['headers']
                # del params['detectors']
                # del params['gses']
                yield copy.copy(self.__dict__)  # shallow copy is enough

    def validate_comb(self, current_comb, num=0):
        """Get valid combination

        Parameters
        ----------
        current_comb
        num

        Returns
        -------

        """
        if (current_comb['dataset_name'] in [
            'DS10_UNB_IDS/DS11-srcIP_192.168.10.5', 'DS10_UNB_IDS/DS12-srcIP_192.168.10.8',
            'DS10_UNB_IDS/DS13-srcIP_192.168.10.9', 'DS10_UNB_IDS/DS14-srcIP_192.168.10.14',
            'DS10_UNB_IDS/DS15-srcIP_192.168.10.15',

            'DS10_UNB_IDS/DS16-srcIP_172.16.0.1',

            ## All anomaly in each dataset of UNB
            'DS10_UNB_IDS/Tuesday/Tuesday-srcIP_172.16.0.1',
            'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1',
            'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1',
            'DS10_UNB_IDS/Thursday/Thursday-srcIP_192.168.10.8',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.5',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.8',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.9',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.12',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.14',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.15',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.17',
            'DS10_UNB_IDS/Friday/Friday-srcIP_205.174.165.73',
            'DS10_UNB_IDS/Friday/Friday-srcIP_172.16.0.1',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.50',

            # indivdual anomaly
            # TUESDAY
            'DS10_UNB_IDS/Tuesday/Tuesday-srcIP_172.16.0.1/FTP-PATATOR',
            'DS10_UNB_IDS/Tuesday/Tuesday-srcIP_172.16.0.1/SSH-PATATOR',
            # WEDNESDAY
            'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-SLOWLORIS',
            'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-SLOWHTTPTEST',
            'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-HULK',
            'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/DOS-GOLDENEYE',
            'DS10_UNB_IDS/Wednesday/Wednesday-srcIP_172.16.0.1/HEARTBLEED',
            # Thursday
            'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1/WEB-ATTACK-BRUTE-FORCE',
            'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1/WEB-ATTACK-XSS',
            'DS10_UNB_IDS/Thursday/Thursday-srcIP_172.16.0.1/WEB-ATTACK-SQL-INJECTION',
            'DS10_UNB_IDS/Thursday/Thursday-srcIP_192.168.10.8/INFILTRATION',
            # Friday
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.5/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.8/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.9/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.12/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.14/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.15/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.17/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_205.174.165.73/BOT',
            'DS10_UNB_IDS/Friday/Friday-srcIP_172.16.0.1/PORTSCAN',
            'DS10_UNB_IDS/Friday/Friday-srcIP_172.16.0.1/DDOS',
            'DS10_UNB_IDS/Friday/Friday-srcIP_192.168.10.50/DDOS',

        ] and current_comb['subflow'] == True and current_comb['data_cat'] == 'AGMT'):
            valid = True
        elif (current_comb['dataset_name'] in [
            'DS20_PU_SMTV/DS21-srcIP_10.42.0.1',
            'DS30_OCS_IoT/DS31-srcIP_192.168.0.13',
            'DS40_CTU_IoT/DS41-srcIP_10.0.2.15',
            'DS50_MAWI_WIDE/DS51-srcIP_202.171.168.50',
            'DS60_UChi_IoT/DS61-srcIP_192.168.143.20', 'DS60_UChi_IoT/DS62-srcIP_192.168.143.42',
            'DS60_UChi_IoT/DS63-srcIP_192.168.143.43', 'DS60_UChi_IoT/DS64-srcIP_192.168.143.48',

            # # CTU-IoT-23: new IoT dataset
            'DS70_CTU_IoT/DS71-srcIP_192.168.100.113',  # no normal flows
            'DS70_CTU_IoT/DS71-srcIP_192.168.1.198',

        ] and current_comb['subflow'] == True and current_comb['data_cat'] == 'INDV'):
            valid = True
        else:
            valid = False

        if valid:
            num += 1

        return valid, num

    def get_combs(self):
        """Get total combinations and valid combinations

        Returns
        -------

        """
        self.num_dataset_combs = len(self.datasets) * len(self.data_cats) * len(self.subflows) * len(self.headers)
        print(f'num_dataset_combs: {self.num_dataset_combs} = {len(self.datasets)}(datasets)*'
              f'{len(self.data_cats)}(data_cats)*{len(self.subflows)}(subflows)*{len(self.headers)}(headers)')

        self.num_detector_combs = len(self.detectors) * len(self.gses)
        print(f'num_detector_combs: {self.num_detector_combs} = {len(self.detectors)}(detectors)*'
              f'{len(self.gses)}(gses)')

        self.num_combs = 0  # valid params combinations
        _combs = []
        for i, comb_i in enumerate(self.get_one_comb()):
            valid, self.num_combs = self.validate_comb(comb_i, self.num_combs)
            if not valid:
                continue
            else:
                _combs.append(comb_i)

        print(f'total combs: {self.num_dataset_combs * self.num_detector_combs}, valid combs: {self.num_combs}')
        self.combs = _combs

        return self.combs


@execute_time
@func_notation
def main(detector_name="GMM", start_time=time.strftime(TIME_FORMAT, time.localtime())):
    """Main function includes two steps:
        1. Get results by detector_name with all combinations of parameters
        2. Save the results to txt, and further transform them to csv, xlsx, and latex

    Parameters
    ----------
    detector_name : string
        outlier detection algorithm

    start_time:
        start time of the application

    """
    # obtain all combinations of experimental parameters
    expt = EXPT(detector_name=detector_name)
    combs = expt.get_combs()  # get valid parameters combinations.
    num_combs = expt.num_combs

    if not pth.exists(expt.opt_dir):
        os.makedirs(expt.opt_dir)
    raw_txt_file = f'{expt.opt_dir}/{detector_name}_result_{start_time}.txt'
    print(f"raw_txt_file: {raw_txt_file}")
    # save all experimental results to disk
    results = OrderedDict()

    # stored processed_pcaps to avoid parsing the same pcap again
    processed_pcaps = []
    for i, current_comb in enumerate(combs):
        try:
            progress_bar(i, num=num_combs)
            if current_comb['dataset_name'] in processed_pcaps:
                current_comb['overwrite'] = False
            else:
                # default True. False: use the previous flows and labels extracted from pcaps.
                current_comb['overwrite'] = True
                processed_pcaps.append(current_comb['dataset_name'])

            # different algorithms have their own datasets to avoid access issues caused by different algorithms
            # simultaneously
            # current_comb will be updated, such as in_dir and out_dir in every loop.
            current_comb = copy_dataset(current_comb)
            if i == 0:
                # remove old file to get more space
                rm_outdated_files(current_comb['ipt_dir'], dur=7 * (24 * 60 * 60))
                rm_outdated_files(current_comb['opt_dir'], dur=7 * (24 * 60 * 60))

            # 1. obtain results with each combination of parameters, which includes three steps:
            # 1) data process 2) build and evaluate the ndm 3) display and save the resutls to files
            mf = MainFactory(params=current_comb)
            current_result = mf.run()

            # get key and save current results to all results
            dt = current_comb.get('detector_name')
            gs = current_comb.get('gs')
            sf = str(current_comb.get('subflow')) + '-q_flow_dur:' + str(current_comb.get('q_flow_dur')) \
                 + '-interval:' + str(current_comb.get('subflow_interval'))
            hdr = current_comb.get('header')
            dc = current_comb.get('data_cat')
            ds = current_comb.get('dataset_name')
            # keys list:
            keys = [dt, f'gs:{gs}', f'sf:{sf}', f'hdr:{hdr}', dc, ds]
            print(f'{i}, keys:{keys}')
            # mf.data_lst already decides the order of aucs
            # save current experimental results to all results, i.e., update the OrderedDict of the results
            set_dict(results, keys=keys, value=current_result)

            # 2. save results
            # Although saving results everytime does redundant work, it can check results immediately
            # and avoid missing out of any data due to any unknown reason (e.g., out of memory, which can't be
            # captured by try-catch statements).
            save_total_results(copy.deepcopy(results), output_file=raw_txt_file, detector_name=detector_name,
                               params=current_comb)

            print(f'{i}, current results: {current_result}')
            del mf  # delete mf to release memory
        except (MemoryError, KeyboardInterrupt, Exception) as e:
            print(f"{i}/{num_combs},{e}")
            traceback.print_tb(e.__traceback__)  # cann't be saved to txt; otherwise it is given a file parameter
            continue
        except:  # can catch any exceptions
            print(f"{i}/{num_combs}, unexpected error: {sys.exc_info()}")
            continue
    # print the last progress bar
    progress_bar(i + 1, num=num_combs)  # all experiments are conducted.
    print('finished.')

    return 0


@func_notation
def parse_cmd_args():
    """Parse commandline parameters

    Returns:
        args: parsed commandline parameters
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--detector", help="outlier detection algorithm", default="PCA")
    parser.add_argument("-t", "--time", help="start time of the application",
                        default=time.strftime(TIME_FORMAT, time.localtime()))
    args = parser.parse_args()
    print(f"args: {args}")

    return args


if __name__ == '__main__':
    args = parse_cmd_args()
    main(detector_name=args.detector.upper(), start_time=args.time)
